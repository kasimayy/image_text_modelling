{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Training\n",
    "\n",
    "This is a partial implementation of \"Show and Tell: A Neural Image Caption Generator\" (http://arxiv.org/abs/1411.4555), borrowing heavily from Andrej Karpathy's NeuralTalk (https://github.com/karpathy/neuraltalk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "from collections import Counter\n",
    "from lasagne.utils import floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the preprocessed dataset containing features extracted by GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open('chestx_with_cnn_features.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count words occuring at least 5 times and construct mapping int <-> word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = Counter()\n",
    "for item in dataset:\n",
    "    tokens = item['tokens']\n",
    "    #for sentence in item['sentences']:\n",
    "    allwords.update(tokens)\n",
    "        \n",
    "vocab = [k for k, v in allwords.items() if v >= 5]\n",
    "vocab.insert(0, '#START#')\n",
    "vocab.append('#END#')\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
    "index_to_word = {i: w for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEQUENCE_LENGTH = 32\n",
    "SEQUENCE_LENGTH = 8\n",
    "MAX_SENTENCE_LENGTH = SEQUENCE_LENGTH - 3 # 1 for image, 1 for start token, 1 for end token\n",
    "BATCH_SIZE = 100\n",
    "CNN_FEATURE_SIZE = 1000\n",
    "EMBEDDING_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns a list of tuples (cnn features, list of words, image ID)\n",
    "def get_data_batch(dataset, size, split='train'):\n",
    "    items = []\n",
    "    \n",
    "    while len(items) < size:\n",
    "        item = random.choice(dataset)\n",
    "        if item['split'] != split:\n",
    "            continue\n",
    "        sentence = item['tokens']\n",
    "        if len(sentence) > MAX_SENTENCE_LENGTH:\n",
    "            continue\n",
    "        items.append((item['cnn features'], sentence, item['imageid']))\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a list of tuples into arrays that can be fed into the network\n",
    "def prep_batch_for_network(batch):\n",
    "    x_cnn = floatX(np.zeros((len(batch), 1000)))\n",
    "    x_sentence = np.zeros((len(batch), SEQUENCE_LENGTH - 1), dtype='int32')\n",
    "    y_sentence = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='int32')\n",
    "    mask = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='bool')\n",
    "\n",
    "    for j, (cnn_features, sentence, _) in enumerate(batch):\n",
    "        x_cnn[j] = cnn_features\n",
    "        i = 0\n",
    "        for word in ['#START#'] + sentence + ['#END#']:\n",
    "            if word in word_to_index:\n",
    "                mask[j, i] = True\n",
    "                y_sentence[j, i] = word_to_index[word]\n",
    "                x_sentence[j, i] = word_to_index[word]\n",
    "                i += 1\n",
    "        #mask[j, 0] = False\n",
    "                \n",
    "    return x_cnn, x_sentence, y_sentence, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embedding maps integer sequence with dim (BATCH_SIZE, SEQUENCE_LENGTH - 1) to \n",
    "# (BATCH_SIZE, SEQUENCE_LENGTH-1, EMBEDDING_SIZE)\n",
    "l_input_sentence = lasagne.layers.InputLayer((BATCH_SIZE, SEQUENCE_LENGTH - 1))\n",
    "l_sentence_embedding = lasagne.layers.EmbeddingLayer(l_input_sentence,\n",
    "                                                     input_size=len(vocab),\n",
    "                                                     output_size=EMBEDDING_SIZE,\n",
    "                                                    )\n",
    "\n",
    "# cnn embedding changes the dimensionality of the representation from 1000 to EMBEDDING_SIZE, \n",
    "# and reshapes to add the time dimension - final dim (BATCH_SIZE, 1, EMBEDDING_SIZE)\n",
    "l_input_cnn = lasagne.layers.InputLayer((BATCH_SIZE, CNN_FEATURE_SIZE))\n",
    "l_cnn_embedding = lasagne.layers.DenseLayer(l_input_cnn, num_units=EMBEDDING_SIZE,\n",
    "                                            nonlinearity=lasagne.nonlinearities.identity)\n",
    "\n",
    "l_cnn_embedding = lasagne.layers.ReshapeLayer(l_cnn_embedding, ([0], 1, [1]))\n",
    "\n",
    "# the two are concatenated to form the RNN input with dim (BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "l_rnn_input = lasagne.layers.ConcatLayer([l_cnn_embedding, l_sentence_embedding])\n",
    "\n",
    "l_dropout_input = lasagne.layers.DropoutLayer(l_rnn_input, p=0.5)\n",
    "l_lstm = lasagne.layers.LSTMLayer(l_dropout_input,\n",
    "                                  num_units=EMBEDDING_SIZE,\n",
    "                                  unroll_scan=True,\n",
    "                                  grad_clipping=5.)\n",
    "l_dropout_output = lasagne.layers.DropoutLayer(l_lstm, p=0.5)\n",
    "\n",
    "# the RNN output is reshaped to combine the batch and time dimensions\n",
    "# dim (BATCH_SIZE * SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "l_shp = lasagne.layers.ReshapeLayer(l_dropout_output, (-1, EMBEDDING_SIZE))\n",
    "\n",
    "# decoder is a fully connected layer with one output unit for each word in the vocabulary\n",
    "l_decoder = lasagne.layers.DenseLayer(l_shp, num_units=len(vocab), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# finally, the separation between batch and time dimension is restored\n",
    "l_out = lasagne.layers.ReshapeLayer(l_decoder, (BATCH_SIZE, SEQUENCE_LENGTH, len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define symbolic variables for the various inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn feature vector\n",
    "x_cnn_sym = T.matrix()\n",
    "\n",
    "# sentence encoded as sequence of integer word tokens\n",
    "x_sentence_sym = T.imatrix()\n",
    "\n",
    "# mask defines which elements of the sequence should be predicted\n",
    "mask_sym = T.imatrix()\n",
    "\n",
    "# ground truth for the RNN output\n",
    "y_sentence_sym = T.imatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = lasagne.layers.get_output(l_out, {\n",
    "                l_input_sentence: x_sentence_sym,\n",
    "                l_input_cnn: x_cnn_sym\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cross_ent(net_output, mask, targets):\n",
    "    # Helper function to calculate the cross entropy error\n",
    "    preds = T.reshape(net_output, (-1, len(vocab)))\n",
    "    targets = T.flatten(targets)\n",
    "    cost = T.nnet.categorical_crossentropy(preds, targets)[T.flatten(mask).nonzero()]\n",
    "    return cost\n",
    "\n",
    "loss = T.mean(calc_cross_ent(output, mask_sym, y_sentence_sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n"
     ]
    }
   ],
   "source": [
    "MAX_GRAD_NORM = 15\n",
    "\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "all_grads = T.grad(loss, all_params)\n",
    "all_grads = [T.clip(g, -5, 5) for g in all_grads]\n",
    "all_grads, norm = lasagne.updates.total_norm_constraint(\n",
    "    all_grads, MAX_GRAD_NORM, return_norm=True)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_params, learning_rate=0.001)\n",
    "\n",
    "f_train = theano.function([x_cnn_sym, x_sentence_sym, mask_sym, y_sentence_sym],\n",
    "                          [loss, norm],\n",
    "                          updates=updates\n",
    "                         )\n",
    "\n",
    "f_val = theano.function([x_cnn_sym, x_sentence_sym, mask_sym, y_sentence_sym], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss_train: 5.0068722096, norm: 1.60650365846\n",
      "Val loss: 4.78040875889\n",
      "Iteration 250, loss_train: 1.31824635792, norm: 0.40263765218\n",
      "Val loss: 1.44438871429\n",
      "Iteration 500, loss_train: 1.11125985044, norm: 0.340303906134\n",
      "Val loss: 1.15957694202\n",
      "Iteration 750, loss_train: 1.15846865202, norm: 0.390729665161\n",
      "Val loss: 1.06037550025\n",
      "Iteration 1000, loss_train: 1.02682366324, norm: 0.431634866221\n",
      "Val loss: 1.13738131571\n",
      "Iteration 1250, loss_train: 1.04995929904, norm: 0.410991962603\n",
      "Val loss: 1.05163230244\n",
      "Iteration 1500, loss_train: 0.980747535122, norm: 0.429945388656\n",
      "Val loss: 0.921374728624\n",
      "Iteration 1750, loss_train: 1.06254477603, norm: 0.402618007485\n",
      "Val loss: 1.0865428012\n",
      "Iteration 2000, loss_train: 1.02636210961, norm: 0.405532430591\n",
      "Val loss: 1.03462347018\n",
      "Iteration 2250, loss_train: 1.05907392121, norm: 0.452020978426\n",
      "Val loss: 1.09051302322\n",
      "Iteration 2500, loss_train: 0.959805754225, norm: 0.365513104615\n",
      "Val loss: 1.02762033326\n",
      "Iteration 2750, loss_train: 0.917749137338, norm: 0.389652833619\n",
      "Val loss: 0.985430952172\n",
      "Iteration 3000, loss_train: 1.03198113056, norm: 0.392950487905\n",
      "Val loss: 0.999154678184\n",
      "Iteration 3250, loss_train: 1.03954563828, norm: 0.360303394313\n",
      "Val loss: 1.12931985577\n",
      "Iteration 3500, loss_train: 1.01307122564, norm: 0.437572062006\n",
      "Val loss: 1.10505775858\n",
      "Iteration 3750, loss_train: 0.938060865401, norm: 0.375259179943\n",
      "Val loss: 1.00768667393\n",
      "Iteration 4000, loss_train: 0.940236761347, norm: 0.394757162312\n",
      "Val loss: 0.994143977616\n",
      "Iteration 4250, loss_train: 0.900816203286, norm: 0.357989571497\n",
      "Val loss: 1.1157055105\n",
      "Iteration 4500, loss_train: 0.966189331007, norm: 0.381095555347\n",
      "Val loss: 1.07640759008\n",
      "Iteration 4750, loss_train: 0.882859980635, norm: 0.370111786849\n",
      "Val loss: 1.11133430269\n",
      "Iteration 5000, loss_train: 0.920402357962, norm: 0.350911264311\n",
      "Val loss: 1.05895758931\n",
      "Iteration 5250, loss_train: 0.886614208305, norm: 0.374039670368\n",
      "Val loss: 1.04065857238\n",
      "Iteration 5500, loss_train: 0.91094678704, norm: 0.380054250521\n",
      "Val loss: 1.06680958305\n",
      "Iteration 5750, loss_train: 0.957071616667, norm: 0.432196844866\n",
      "Val loss: 0.957178658616\n",
      "Iteration 6000, loss_train: 0.878177816909, norm: 0.560568211757\n",
      "Val loss: 1.07152294843\n",
      "Iteration 6250, loss_train: 0.936603531431, norm: 0.575542300977\n",
      "Val loss: 0.970161187748\n",
      "Iteration 6500, loss_train: 0.847210985307, norm: 0.435721830321\n",
      "Val loss: 1.05477379703\n",
      "Iteration 6750, loss_train: 0.846628988797, norm: 0.372571539538\n",
      "Val loss: 1.13295904701\n",
      "Iteration 7000, loss_train: 0.831335092978, norm: 0.370303304609\n",
      "Val loss: 1.08168060741\n",
      "Iteration 7250, loss_train: 0.907874880626, norm: 0.751414970238\n",
      "Val loss: 0.947909935154\n",
      "Iteration 7500, loss_train: 0.879448926208, norm: 0.373914418566\n",
      "Val loss: 1.07783877663\n",
      "Iteration 7750, loss_train: 0.789409819031, norm: 0.572130313449\n",
      "Val loss: 1.10797690437\n",
      "Iteration 8000, loss_train: 0.904844422561, norm: 0.429851382303\n",
      "Val loss: 1.05620368991\n",
      "Iteration 8250, loss_train: 0.861280997571, norm: 0.766981543081\n",
      "Val loss: 1.11911576634\n",
      "Iteration 8500, loss_train: 0.940503413518, norm: 1.31448183986\n",
      "Val loss: 1.06636112903\n",
      "Iteration 8750, loss_train: 0.791693564304, norm: 0.462560231182\n",
      "Val loss: 1.1171886978\n",
      "Iteration 9000, loss_train: 0.848998063016, norm: 0.445977262697\n",
      "Val loss: 1.18890268307\n",
      "Iteration 9250, loss_train: 0.763724657548, norm: 0.543274650941\n",
      "Val loss: 1.1233929115\n",
      "Iteration 9500, loss_train: 0.805213375286, norm: 0.643342844941\n",
      "Val loss: 1.01854493346\n",
      "Iteration 9750, loss_train: 0.801509320323, norm: 0.455128376789\n",
      "Val loss: 1.0444170917\n",
      "Iteration 10000, loss_train: 0.857990454469, norm: 0.461078154183\n",
      "Val loss: 0.93737293705\n",
      "Iteration 10250, loss_train: 0.880578743492, norm: 0.617181191054\n",
      "Val loss: 1.21141065599\n",
      "Iteration 10500, loss_train: 0.873923321643, norm: 1.31118303691\n",
      "Val loss: 1.11237268161\n",
      "Iteration 10750, loss_train: 0.858844530515, norm: 0.663656558663\n",
      "Val loss: 1.10720577474\n",
      "Iteration 11000, loss_train: 0.869475008601, norm: 0.532911917202\n",
      "Val loss: 1.295924936\n",
      "Iteration 11250, loss_train: 0.777264196698, norm: 0.522224422223\n",
      "Val loss: 1.12656055082\n",
      "Iteration 11500, loss_train: 0.831538691388, norm: 0.447076344747\n",
      "Val loss: 1.34090556401\n",
      "Iteration 11750, loss_train: 0.818537959063, norm: 0.491103725367\n",
      "Val loss: 1.25270040688\n",
      "Iteration 12000, loss_train: 0.746848973542, norm: 0.994149663779\n",
      "Val loss: 1.14166999331\n",
      "Iteration 12250, loss_train: 0.719648251432, norm: 0.390456402843\n",
      "Val loss: 1.17108862415\n",
      "Iteration 12500, loss_train: 0.797486267219, norm: 0.460871820896\n",
      "Val loss: 1.27432492759\n",
      "Iteration 12750, loss_train: 0.796360853554, norm: 0.588036257033\n",
      "Val loss: 1.26672044597\n",
      "Iteration 13000, loss_train: 0.760602812901, norm: 0.473381117397\n",
      "Val loss: 1.19046304796\n",
      "Iteration 13250, loss_train: 0.746181803792, norm: 0.395835691094\n",
      "Val loss: 1.23177712945\n",
      "Iteration 13500, loss_train: 0.751118773087, norm: 1.49933000488\n",
      "Val loss: 1.16438822284\n",
      "Iteration 13750, loss_train: 0.743423599119, norm: 0.578939820446\n",
      "Val loss: 1.32613026541\n",
      "Iteration 14000, loss_train: 0.726684080566, norm: 0.620349638784\n",
      "Val loss: 1.18535931303\n",
      "Iteration 14250, loss_train: 0.701320038822, norm: 0.501322832851\n",
      "Val loss: 1.40540632556\n",
      "Iteration 14500, loss_train: 0.814868909012, norm: 0.532739306849\n",
      "Val loss: 1.14941254846\n",
      "Iteration 14750, loss_train: 0.7634912959, norm: 0.687237065051\n",
      "Val loss: 1.14943375282\n",
      "Iteration 15000, loss_train: 0.734287171235, norm: 0.872058568362\n",
      "Val loss: 1.23474604114\n",
      "Iteration 15250, loss_train: 0.695460425726, norm: 0.521860807268\n",
      "Val loss: 1.41739963771\n",
      "Iteration 15500, loss_train: 0.747105334822, norm: 0.740354997789\n",
      "Val loss: 1.2679257177\n",
      "Iteration 15750, loss_train: 0.700825102078, norm: 0.474135207965\n",
      "Val loss: 1.14748123188\n",
      "Iteration 16000, loss_train: 0.777675464323, norm: 0.540126205648\n",
      "Val loss: 1.2593973614\n",
      "Iteration 16250, loss_train: 0.749646432394, norm: 0.465401931972\n",
      "Val loss: 1.27160191591\n",
      "Iteration 16500, loss_train: 0.779863663733, norm: 0.967472358207\n",
      "Val loss: 1.29281512583\n",
      "Iteration 16750, loss_train: 0.710651988295, norm: 0.572791158946\n",
      "Val loss: 1.21934440467\n",
      "Iteration 17000, loss_train: 0.700441302109, norm: 0.843690925694\n",
      "Val loss: 1.41720311473\n",
      "Iteration 17250, loss_train: 0.719581723427, norm: 0.880615984715\n",
      "Val loss: 1.37077103747\n",
      "Iteration 17500, loss_train: 0.639551060237, norm: 0.893192642339\n",
      "Val loss: 1.21646751127\n",
      "Iteration 17750, loss_train: 0.713941324144, norm: 0.69990585041\n",
      "Val loss: 1.11684118322\n",
      "Iteration 18000, loss_train: 0.653015579027, norm: 0.544674689403\n",
      "Val loss: 1.49554110007\n",
      "Iteration 18250, loss_train: 0.663947968781, norm: 0.455213640023\n",
      "Val loss: 1.27946892314\n",
      "Iteration 18500, loss_train: 0.750771630774, norm: 1.33051016015\n",
      "Val loss: 1.3430176147\n",
      "Iteration 18750, loss_train: 0.685142727658, norm: 0.484634219325\n",
      "Val loss: 1.3876538861\n",
      "Iteration 19000, loss_train: 0.675836932719, norm: 0.718839503464\n",
      "Val loss: 1.15615475128\n",
      "Iteration 19250, loss_train: 0.700161260109, norm: 0.741164004722\n",
      "Val loss: 1.27620346118\n",
      "Iteration 19500, loss_train: 0.710816485032, norm: 0.474173688871\n",
      "Val loss: 1.50486801916\n",
      "Iteration 19750, loss_train: 0.6060530363, norm: 0.758770906633\n",
      "Val loss: 1.14795774669\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(20000):\n",
    "    x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(get_data_batch(dataset, BATCH_SIZE))\n",
    "    loss_train, norm = f_train(x_cnn, x_sentence, mask, y_sentence)\n",
    "    if not iteration % 250:\n",
    "        print('Iteration {}, loss_train: {}, norm: {}'.format(iteration, loss_train, norm))\n",
    "        try:\n",
    "            batch = get_data_batch(dataset, BATCH_SIZE, split='val')\n",
    "            x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(batch)\n",
    "            loss_val = f_val(x_cnn, x_sentence, mask, y_sentence)\n",
    "            print('Val loss: {}'.format(loss_val))\n",
    "        except IndexError:\n",
    "            continue        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_values = lasagne.layers.get_all_param_values(l_out)\n",
    "d = {'param values': param_values,\n",
    "     'vocab': vocab,\n",
    "     'word_to_index': word_to_index,\n",
    "     'index_to_word': index_to_word,\n",
    "    }\n",
    "pickle.dump(d, open('lstm_chestx_trained.pkl','w'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
